{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverages a state-of-the-art Transformer model for high-quality translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 15:52:49.543869: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 500\n",
      "Number of unique input tokens: 23\n",
      "Number of unique output tokens: 68\n",
      "Max sequence length for inputs: 20\n",
      "Max sequence length for outputs: 13\n",
      "Epoch 1/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 561ms/step - accuracy: 0.0479 - loss: 1.5643 - val_accuracy: 0.0769 - val_loss: 1.7360\n",
      "Epoch 2/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 325ms/step - accuracy: 0.0646 - loss: 1.4353 - val_accuracy: 0.0769 - val_loss: 1.6781\n",
      "Epoch 3/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 324ms/step - accuracy: 0.0569 - loss: 1.4252 - val_accuracy: 0.0769 - val_loss: 1.6889\n",
      "Epoch 4/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 323ms/step - accuracy: 0.0462 - loss: 1.4910 - val_accuracy: 0.0769 - val_loss: 1.6336\n",
      "Epoch 5/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 331ms/step - accuracy: 0.0310 - loss: 1.6362 - val_accuracy: 0.0769 - val_loss: 1.7048\n",
      "Epoch 6/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 341ms/step - accuracy: 0.0231 - loss: 1.9191 - val_accuracy: 0.0000e+00 - val_loss: 2.1123\n",
      "Epoch 7/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 321ms/step - accuracy: 0.0181 - loss: 2.3730 - val_accuracy: 0.0054 - val_loss: 2.3861\n",
      "Epoch 8/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 348ms/step - accuracy: 0.0165 - loss: 3.0404 - val_accuracy: 0.0462 - val_loss: 2.8843\n",
      "Epoch 9/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 336ms/step - accuracy: 0.0173 - loss: 3.8049 - val_accuracy: 0.0769 - val_loss: 3.9900\n",
      "Epoch 10/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 362ms/step - accuracy: 0.0173 - loss: 5.5490 - val_accuracy: 0.0292 - val_loss: 8.2741\n",
      "Epoch 11/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 313ms/step - accuracy: 0.0163 - loss: 8.7257 - val_accuracy: 0.0100 - val_loss: 15.1611\n",
      "Epoch 12/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 376ms/step - accuracy: 0.0135 - loss: 19.5302 - val_accuracy: 0.0069 - val_loss: 28.6553\n",
      "Epoch 13/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 384ms/step - accuracy: 0.0208 - loss: 30.4135 - val_accuracy: 0.0085 - val_loss: 50.7336\n",
      "Epoch 14/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 354ms/step - accuracy: 0.0137 - loss: 47.3352 - val_accuracy: 0.0769 - val_loss: 68.3714\n",
      "Epoch 15/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 355ms/step - accuracy: 0.0169 - loss: 55.2632 - val_accuracy: 0.0000e+00 - val_loss: 81.1642\n",
      "Epoch 16/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 401ms/step - accuracy: 0.0162 - loss: 80.3226 - val_accuracy: 0.0769 - val_loss: 110.7450\n",
      "Epoch 17/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 591ms/step - accuracy: 0.0194 - loss: 115.7745 - val_accuracy: 0.0215 - val_loss: 199.0763\n",
      "Epoch 18/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 599ms/step - accuracy: 0.0129 - loss: 151.2640 - val_accuracy: 0.0038 - val_loss: 224.2719\n",
      "Epoch 19/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 571ms/step - accuracy: 0.0183 - loss: 187.5383 - val_accuracy: 7.6923e-04 - val_loss: 298.7387\n",
      "Epoch 20/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 629ms/step - accuracy: 0.0144 - loss: 243.6719 - val_accuracy: 0.0092 - val_loss: 361.7625\n",
      "Epoch 21/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 555ms/step - accuracy: 0.0144 - loss: 305.8669 - val_accuracy: 0.0131 - val_loss: 358.5261\n",
      "Epoch 22/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 400ms/step - accuracy: 0.0154 - loss: 311.1067 - val_accuracy: 0.0038 - val_loss: 375.3842\n",
      "Epoch 23/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 449ms/step - accuracy: 0.0194 - loss: 320.4265 - val_accuracy: 0.0000e+00 - val_loss: 430.8578\n",
      "Epoch 24/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 416ms/step - accuracy: 0.0158 - loss: 377.3975 - val_accuracy: 0.0046 - val_loss: 588.5175\n",
      "Epoch 25/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 437ms/step - accuracy: 0.0088 - loss: 513.3495 - val_accuracy: 0.0769 - val_loss: 621.5218\n",
      "Epoch 26/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 420ms/step - accuracy: 0.0210 - loss: 519.4309 - val_accuracy: 0.0054 - val_loss: 850.3730\n",
      "Epoch 27/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 507ms/step - accuracy: 0.0160 - loss: 644.0350 - val_accuracy: 0.0069 - val_loss: 722.7715\n",
      "Epoch 28/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 403ms/step - accuracy: 0.0304 - loss: 646.5055 - val_accuracy: 0.0223 - val_loss: 984.8416\n",
      "Epoch 29/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 389ms/step - accuracy: 0.0171 - loss: 870.8563 - val_accuracy: 0.0000e+00 - val_loss: 1298.7921\n",
      "Epoch 30/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 422ms/step - accuracy: 0.0175 - loss: 981.1387 - val_accuracy: 7.6923e-04 - val_loss: 1060.3508\n",
      "Epoch 31/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 438ms/step - accuracy: 0.0108 - loss: 859.6292 - val_accuracy: 0.0462 - val_loss: 1137.6122\n",
      "Epoch 32/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 545ms/step - accuracy: 0.0160 - loss: 1086.8647 - val_accuracy: 0.0100 - val_loss: 1785.2333\n",
      "Epoch 33/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 471ms/step - accuracy: 0.0096 - loss: 1527.7289 - val_accuracy: 0.0062 - val_loss: 1736.1989\n",
      "Epoch 34/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 442ms/step - accuracy: 0.0167 - loss: 1501.8495 - val_accuracy: 0.0131 - val_loss: 1848.0782\n",
      "Epoch 35/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 475ms/step - accuracy: 0.0129 - loss: 1220.9429 - val_accuracy: 0.0292 - val_loss: 1838.0581\n",
      "Epoch 36/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 463ms/step - accuracy: 0.0171 - loss: 1445.1000 - val_accuracy: 0.0223 - val_loss: 1972.7639\n",
      "Epoch 37/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 413ms/step - accuracy: 0.0206 - loss: 1610.0492 - val_accuracy: 7.6923e-04 - val_loss: 2419.5479\n",
      "Epoch 38/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 393ms/step - accuracy: 0.0112 - loss: 1920.4020 - val_accuracy: 0.0100 - val_loss: 2529.8787\n",
      "Epoch 39/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 393ms/step - accuracy: 0.0156 - loss: 2086.5996 - val_accuracy: 0.0046 - val_loss: 2841.1663\n",
      "Epoch 40/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 377ms/step - accuracy: 0.0177 - loss: 2310.5471 - val_accuracy: 0.5392 - val_loss: 3083.5894\n",
      "Epoch 41/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 367ms/step - accuracy: 0.0446 - loss: 2416.6760 - val_accuracy: 0.0223 - val_loss: 3587.6509\n",
      "Epoch 42/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 397ms/step - accuracy: 0.0113 - loss: 2855.1931 - val_accuracy: 0.0000e+00 - val_loss: 3918.9985\n",
      "Epoch 43/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 446ms/step - accuracy: 0.0154 - loss: 3091.1191 - val_accuracy: 0.0100 - val_loss: 3769.9077\n",
      "Epoch 44/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 525ms/step - accuracy: 0.0177 - loss: 2683.5596 - val_accuracy: 0.0023 - val_loss: 3397.1829\n",
      "Epoch 45/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 493ms/step - accuracy: 0.0142 - loss: 2906.6877 - val_accuracy: 0.0085 - val_loss: 4885.9277\n",
      "Epoch 46/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 499ms/step - accuracy: 0.0129 - loss: 4030.9170 - val_accuracy: 0.0046 - val_loss: 5217.8594\n",
      "Epoch 47/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 431ms/step - accuracy: 0.0194 - loss: 4231.1284 - val_accuracy: 0.0215 - val_loss: 5767.4067\n",
      "Epoch 48/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 501ms/step - accuracy: 0.0167 - loss: 3830.3730 - val_accuracy: 0.0292 - val_loss: 4403.8726\n",
      "Epoch 49/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 425ms/step - accuracy: 0.0098 - loss: 4046.7319 - val_accuracy: 0.0000e+00 - val_loss: 5597.2920\n",
      "Epoch 50/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 425ms/step - accuracy: 0.0154 - loss: 4760.2061 - val_accuracy: 0.0015 - val_loss: 6552.6211\n",
      "Epoch 51/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 480ms/step - accuracy: 0.0546 - loss: 5252.9570 - val_accuracy: 0.0223 - val_loss: 7127.5117\n",
      "Epoch 52/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 804ms/step - accuracy: 0.0127 - loss: 5178.8848 - val_accuracy: 0.0462 - val_loss: 6086.4307\n",
      "Epoch 53/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 818ms/step - accuracy: 0.0142 - loss: 5269.1851 - val_accuracy: 0.0038 - val_loss: 5760.1313\n",
      "Epoch 54/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 559ms/step - accuracy: 0.0096 - loss: 4445.0645 - val_accuracy: 0.0462 - val_loss: 6630.5176\n",
      "Epoch 55/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 543ms/step - accuracy: 0.0183 - loss: 5603.6025 - val_accuracy: 0.0292 - val_loss: 7826.0669\n",
      "Epoch 56/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 442ms/step - accuracy: 0.0167 - loss: 6293.4814 - val_accuracy: 0.0131 - val_loss: 8791.5898\n",
      "Epoch 57/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 471ms/step - accuracy: 0.0188 - loss: 7008.7568 - val_accuracy: 7.6923e-04 - val_loss: 9439.5918\n",
      "Epoch 58/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 492ms/step - accuracy: 0.0129 - loss: 7248.5430 - val_accuracy: 0.0000e+00 - val_loss: 9696.5850\n",
      "Epoch 59/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 509ms/step - accuracy: 0.0125 - loss: 7387.7500 - val_accuracy: 0.0077 - val_loss: 9585.4678\n",
      "Epoch 60/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 443ms/step - accuracy: 0.0177 - loss: 7515.2031 - val_accuracy: 0.0131 - val_loss: 10835.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 382ms/step - accuracy: 0.0585 - loss: 8210.0977 - val_accuracy: 0.0000e+00 - val_loss: 10312.1475\n",
      "Epoch 62/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 367ms/step - accuracy: 0.0167 - loss: 8131.3125 - val_accuracy: 0.0292 - val_loss: 11573.3184\n",
      "Epoch 63/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 413ms/step - accuracy: 0.0073 - loss: 10043.2129 - val_accuracy: 0.0769 - val_loss: 13682.3721\n",
      "Epoch 64/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 410ms/step - accuracy: 0.0196 - loss: 10127.0801 - val_accuracy: 0.0131 - val_loss: 11812.1562\n",
      "Epoch 65/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 444ms/step - accuracy: 0.0138 - loss: 8906.8984 - val_accuracy: 0.0023 - val_loss: 12078.2598\n",
      "Epoch 66/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 417ms/step - accuracy: 0.0138 - loss: 9976.9648 - val_accuracy: 0.0131 - val_loss: 14327.2236\n",
      "Epoch 67/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 556ms/step - accuracy: 0.0181 - loss: 11365.5840 - val_accuracy: 0.0100 - val_loss: 17189.2910\n",
      "Epoch 68/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 434ms/step - accuracy: 0.0133 - loss: 12188.0195 - val_accuracy: 0.0062 - val_loss: 14164.2373\n",
      "Epoch 69/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 401ms/step - accuracy: 0.0108 - loss: 11433.4375 - val_accuracy: 0.0038 - val_loss: 13127.6035\n",
      "Epoch 70/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 421ms/step - accuracy: 0.0190 - loss: 11688.3760 - val_accuracy: 0.0000e+00 - val_loss: 17275.5703\n",
      "Epoch 71/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 389ms/step - accuracy: 0.0573 - loss: 13321.9873 - val_accuracy: 0.0092 - val_loss: 17690.3809\n",
      "Epoch 72/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 405ms/step - accuracy: 0.0144 - loss: 13919.4404 - val_accuracy: 0.0223 - val_loss: 17876.7012\n",
      "Epoch 73/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 461ms/step - accuracy: 0.0146 - loss: 13232.0938 - val_accuracy: 0.0292 - val_loss: 18156.9941\n",
      "Epoch 74/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 468ms/step - accuracy: 0.0138 - loss: 14723.0508 - val_accuracy: 0.0031 - val_loss: 21049.5391\n",
      "Epoch 75/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 621ms/step - accuracy: 0.0098 - loss: 14644.5488 - val_accuracy: 0.0038 - val_loss: 15189.9102\n",
      "Epoch 76/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 553ms/step - accuracy: 0.0167 - loss: 14210.3838 - val_accuracy: 0.0215 - val_loss: 22432.8906\n",
      "Epoch 77/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 507ms/step - accuracy: 0.0187 - loss: 17161.3809 - val_accuracy: 7.6923e-04 - val_loss: 21436.3984\n",
      "Epoch 78/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 454ms/step - accuracy: 0.0146 - loss: 15794.0029 - val_accuracy: 0.0038 - val_loss: 22648.4434\n",
      "Epoch 79/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 456ms/step - accuracy: 0.0112 - loss: 15417.5801 - val_accuracy: 0.0769 - val_loss: 20329.0371\n",
      "Epoch 80/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 420ms/step - accuracy: 0.0196 - loss: 16032.8936 - val_accuracy: 0.0069 - val_loss: 21728.7109\n",
      "Epoch 81/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 419ms/step - accuracy: 0.0131 - loss: 17725.6211 - val_accuracy: 0.0062 - val_loss: 25133.6191\n",
      "Epoch 82/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 406ms/step - accuracy: 0.0600 - loss: 21044.9707 - val_accuracy: 0.0292 - val_loss: 28364.0273\n",
      "Epoch 83/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 435ms/step - accuracy: 0.0169 - loss: 20656.7988 - val_accuracy: 0.0046 - val_loss: 29425.8359\n",
      "Epoch 84/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 449ms/step - accuracy: 0.0140 - loss: 21616.0176 - val_accuracy: 0.0023 - val_loss: 25844.5176\n",
      "Epoch 85/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 488ms/step - accuracy: 0.0173 - loss: 21151.8242 - val_accuracy: 0.0223 - val_loss: 26773.0430\n",
      "Epoch 86/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 488ms/step - accuracy: 0.0125 - loss: 22086.4824 - val_accuracy: 0.0292 - val_loss: 25816.0195\n",
      "Epoch 87/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 532ms/step - accuracy: 0.0144 - loss: 17123.5176 - val_accuracy: 0.0223 - val_loss: 23505.9941\n",
      "Epoch 88/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 531ms/step - accuracy: 0.0154 - loss: 22572.2891 - val_accuracy: 0.0077 - val_loss: 32650.6328\n",
      "Epoch 89/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 527ms/step - accuracy: 0.0140 - loss: 26108.2773 - val_accuracy: 0.0038 - val_loss: 34144.9062\n",
      "Epoch 90/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 487ms/step - accuracy: 0.0110 - loss: 28249.6348 - val_accuracy: 0.0108 - val_loss: 34451.8828\n",
      "Epoch 91/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 536ms/step - accuracy: 0.0165 - loss: 27916.5449 - val_accuracy: 0.0046 - val_loss: 36731.1836\n",
      "Epoch 92/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 537ms/step - accuracy: 0.0683 - loss: 28272.5059 - val_accuracy: 0.0000e+00 - val_loss: 32493.6016\n",
      "Epoch 93/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 596ms/step - accuracy: 0.0133 - loss: 23614.7344 - val_accuracy: 0.0223 - val_loss: 29012.9141\n",
      "Epoch 94/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 538ms/step - accuracy: 0.0156 - loss: 27712.1406 - val_accuracy: 0.0146 - val_loss: 44053.4844\n",
      "Epoch 95/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 512ms/step - accuracy: 0.0150 - loss: 27811.9199 - val_accuracy: 0.0100 - val_loss: 36594.3711\n",
      "Epoch 96/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 771ms/step - accuracy: 0.0142 - loss: 26456.0801 - val_accuracy: 0.0038 - val_loss: 39024.1367\n",
      "Epoch 97/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 442ms/step - accuracy: 0.0131 - loss: 30622.9434 - val_accuracy: 0.0223 - val_loss: 41480.5469\n",
      "Epoch 98/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 496ms/step - accuracy: 0.0194 - loss: 28446.3652 - val_accuracy: 0.0146 - val_loss: 32003.2500\n",
      "Epoch 99/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 488ms/step - accuracy: 0.0106 - loss: 28149.1230 - val_accuracy: 0.0223 - val_loss: 38189.7500\n",
      "Epoch 100/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 507ms/step - accuracy: 0.0162 - loss: 36943.7188 - val_accuracy: 0.0046 - val_loss: 52349.4805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Dropout, LayerNormalization\n",
    "from keras.layers import MultiHeadAttention, GlobalAveragePooling1D, Add\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Load the datasets\n",
    "data_rom = pd.read_csv(\"csv/data_rom_500.csv\", header=None)  # Romanized text (input)\n",
    "data_kh = pd.read_csv(\"csv/data_kh_500.csv\", header=None)    # Khmer text (target)\n",
    "\n",
    "batch_size = 32  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# Prepare datasets for Roman-to-Khmer\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "for input_text in data_rom[0]:  # Romanized text is now the input\n",
    "    input_text = str(input_text).strip()\n",
    "    input_texts.append(input_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "\n",
    "for target_text in data_kh[0]:  # Khmer text is now the target\n",
    "    target_text = '\\t' + str(target_text).strip() + '\\n'\n",
    "    target_texts.append(target_text)\n",
    "    for char in str(target_text):\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "# Sort characters to ensure consistent token indexing\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "# Create token indices\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# Initialize input and output data for the model\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype=\"float32\")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype=\"float32\")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype=\"float32\")\n",
    "\n",
    "# Populate the data arrays\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # Decoder target data is offset by one timestep\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "\n",
    "# Define Transformer block (Self-Attention and Feed Forward)\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    # Self-Attention Layer\n",
    "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n",
    "    attention = Dropout(dropout)(attention)\n",
    "    attention = LayerNormalization()(attention)\n",
    "    \n",
    "    # Feed Forward Layer\n",
    "    ff = Dense(ff_dim, activation=\"relu\")(attention)\n",
    "    ff = Dropout(dropout)(ff)\n",
    "    ff = Dense(inputs.shape[-1])(ff)\n",
    "    return Add()([inputs, ff])\n",
    "\n",
    "def transformer_decoder(inputs, encoder_output, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    # Self-Attention Layer for Decoder\n",
    "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n",
    "    attention = Dropout(dropout)(attention)\n",
    "    attention = LayerNormalization()(attention)\n",
    "    \n",
    "    # Encoder-Decoder Attention Layer\n",
    "    cross_attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(attention, encoder_output)\n",
    "    cross_attention = Dropout(dropout)(cross_attention)\n",
    "    cross_attention = LayerNormalization()(cross_attention)\n",
    "    \n",
    "    # Feed Forward Layer\n",
    "    ff = Dense(ff_dim, activation=\"relu\")(cross_attention)\n",
    "    ff = Dropout(dropout)(ff)\n",
    "    ff = Dense(inputs.shape[-1])(ff)\n",
    "    return Add()([attention, ff])\n",
    "\n",
    "# Transformer Parameters\n",
    "head_size = 64\n",
    "num_heads = 8\n",
    "ff_dim = 256\n",
    "num_layers = 4\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "x = encoder_inputs\n",
    "for _ in range(num_layers):\n",
    "    x = transformer_encoder(x, head_size, num_heads, ff_dim)\n",
    "encoder_output = x\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "x = decoder_inputs\n",
    "for _ in range(num_layers):\n",
    "    x = transformer_decoder(x, encoder_output, head_size, num_heads, ff_dim)\n",
    "decoder_output = Dense(num_decoder_tokens, activation=\"softmax\")(x)\n",
    "\n",
    "# Define the model that combines encoder and decoder\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"transformer_rom_to_khmer.h5\")\n",
    "\n",
    "# Create inference models\n",
    "encoder_model = Model(encoder_inputs, encoder_output)\n",
    "\n",
    "decoder_state_input = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_output = transformer_decoder(decoder_state_input, encoder_output, head_size, num_heads, ff_dim)\n",
    "decoder_output = Dense(num_decoder_tokens, activation=\"softmax\")(decoder_output)\n",
    "decoder_model = Model([decoder_state_input], decoder_output)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to text\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
