{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverages a state-of-the-art Transformer model for high-quality translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 19:27:46.244145: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 500\n",
      "Number of unique input tokens: 23\n",
      "Number of unique output tokens: 68\n",
      "Max sequence length for inputs: 20\n",
      "Max sequence length for outputs: 13\n",
      "Epoch 1/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 553ms/step - accuracy: 0.0362 - loss: 1.6617 - val_accuracy: 0.0769 - val_loss: 1.8048\n",
      "Epoch 2/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 317ms/step - accuracy: 0.0658 - loss: 1.4607 - val_accuracy: 0.0769 - val_loss: 1.7089\n",
      "Epoch 3/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 355ms/step - accuracy: 0.0642 - loss: 1.4226 - val_accuracy: 0.0769 - val_loss: 1.6358\n",
      "Epoch 4/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 401ms/step - accuracy: 0.0525 - loss: 1.4493 - val_accuracy: 0.0769 - val_loss: 1.6393\n",
      "Epoch 5/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 433ms/step - accuracy: 0.0390 - loss: 1.5592 - val_accuracy: 0.0769 - val_loss: 1.6785\n",
      "Epoch 6/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 414ms/step - accuracy: 0.0258 - loss: 1.8082 - val_accuracy: 0.0131 - val_loss: 1.8966\n",
      "Epoch 7/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 395ms/step - accuracy: 0.0210 - loss: 2.2677 - val_accuracy: 0.0769 - val_loss: 2.0327\n",
      "Epoch 8/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 428ms/step - accuracy: 0.0190 - loss: 2.8589 - val_accuracy: 0.0292 - val_loss: 2.3560\n",
      "Epoch 9/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 489ms/step - accuracy: 0.0217 - loss: 3.5190 - val_accuracy: 7.6923e-04 - val_loss: 3.0370\n",
      "Epoch 10/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 457ms/step - accuracy: 0.0187 - loss: 4.9900 - val_accuracy: 0.0046 - val_loss: 8.3737\n",
      "Epoch 11/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 452ms/step - accuracy: 0.0169 - loss: 9.0737 - val_accuracy: 0.0462 - val_loss: 15.5444\n",
      "Epoch 12/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 425ms/step - accuracy: 0.0163 - loss: 18.2098 - val_accuracy: 0.0046 - val_loss: 18.7750\n",
      "Epoch 13/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 467ms/step - accuracy: 0.0156 - loss: 26.0834 - val_accuracy: 7.6923e-04 - val_loss: 49.5772\n",
      "Epoch 14/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 422ms/step - accuracy: 0.0142 - loss: 51.5056 - val_accuracy: 0.0131 - val_loss: 72.7796\n",
      "Epoch 15/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 404ms/step - accuracy: 0.0137 - loss: 63.2456 - val_accuracy: 0.0015 - val_loss: 103.5288\n",
      "Epoch 16/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 446ms/step - accuracy: 0.0194 - loss: 100.3665 - val_accuracy: 0.0100 - val_loss: 178.6830\n",
      "Epoch 17/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 450ms/step - accuracy: 0.0127 - loss: 139.2749 - val_accuracy: 0.0223 - val_loss: 176.9326\n",
      "Epoch 18/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 417ms/step - accuracy: 0.0194 - loss: 165.3026 - val_accuracy: 0.0223 - val_loss: 248.0767\n",
      "Epoch 19/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 429ms/step - accuracy: 0.0152 - loss: 204.8972 - val_accuracy: 0.0023 - val_loss: 276.9703\n",
      "Epoch 20/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 440ms/step - accuracy: 0.0115 - loss: 259.8341 - val_accuracy: 0.0100 - val_loss: 371.9982\n",
      "Epoch 21/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 362ms/step - accuracy: 0.0221 - loss: 287.3305 - val_accuracy: 0.0131 - val_loss: 337.1781\n",
      "Epoch 22/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 407ms/step - accuracy: 0.0133 - loss: 374.6159 - val_accuracy: 0.0108 - val_loss: 509.8607\n",
      "Epoch 23/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 471ms/step - accuracy: 0.0181 - loss: 371.5684 - val_accuracy: 0.0131 - val_loss: 513.7608\n",
      "Epoch 24/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 420ms/step - accuracy: 0.0165 - loss: 497.4555 - val_accuracy: 0.0023 - val_loss: 711.6359\n",
      "Epoch 25/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 380ms/step - accuracy: 0.0196 - loss: 544.5791 - val_accuracy: 0.0085 - val_loss: 792.9880\n",
      "Epoch 26/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 416ms/step - accuracy: 0.0496 - loss: 589.5435 - val_accuracy: 0.0131 - val_loss: 758.2790\n",
      "Epoch 27/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 431ms/step - accuracy: 0.0165 - loss: 633.6093 - val_accuracy: 0.0038 - val_loss: 973.3884\n",
      "Epoch 28/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 402ms/step - accuracy: 0.0133 - loss: 756.4171 - val_accuracy: 0.0054 - val_loss: 938.7866\n",
      "Epoch 29/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 336ms/step - accuracy: 0.0137 - loss: 922.2488 - val_accuracy: 0.0131 - val_loss: 1301.3491\n",
      "Epoch 30/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 339ms/step - accuracy: 0.0175 - loss: 1028.5564 - val_accuracy: 0.0462 - val_loss: 1224.4332\n",
      "Epoch 31/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 397ms/step - accuracy: 0.0133 - loss: 974.3665 - val_accuracy: 0.0000e+00 - val_loss: 1189.8252\n",
      "Epoch 32/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 383ms/step - accuracy: 0.0131 - loss: 999.2466 - val_accuracy: 0.0769 - val_loss: 1270.9702\n",
      "Epoch 33/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 336ms/step - accuracy: 0.0202 - loss: 1184.3264 - val_accuracy: 0.0100 - val_loss: 2127.2500\n",
      "Epoch 34/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 346ms/step - accuracy: 0.0202 - loss: 1690.6653 - val_accuracy: 0.0038 - val_loss: 2051.4648\n",
      "Epoch 35/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 350ms/step - accuracy: 0.0092 - loss: 1741.5778 - val_accuracy: 0.0131 - val_loss: 2393.1809\n",
      "Epoch 36/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 389ms/step - accuracy: 0.0158 - loss: 1879.1798 - val_accuracy: 0.0077 - val_loss: 2545.1052\n",
      "Epoch 37/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 396ms/step - accuracy: 0.0323 - loss: 2022.6558 - val_accuracy: 0.0069 - val_loss: 2425.0042\n",
      "Epoch 38/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 400ms/step - accuracy: 0.0112 - loss: 2033.0646 - val_accuracy: 0.0769 - val_loss: 2767.1946\n",
      "Epoch 39/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 411ms/step - accuracy: 0.0196 - loss: 2252.1697 - val_accuracy: 7.6923e-04 - val_loss: 2765.9438\n",
      "Epoch 40/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 427ms/step - accuracy: 0.0113 - loss: 2327.1946 - val_accuracy: 0.0462 - val_loss: 3198.4729\n",
      "Epoch 41/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 437ms/step - accuracy: 0.0183 - loss: 2567.3215 - val_accuracy: 0.0085 - val_loss: 3639.6677\n",
      "Epoch 42/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 421ms/step - accuracy: 0.0108 - loss: 3349.6753 - val_accuracy: 0.0769 - val_loss: 4214.9902\n",
      "Epoch 43/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 420ms/step - accuracy: 0.0119 - loss: 3700.9490 - val_accuracy: 0.0069 - val_loss: 4612.6787\n",
      "Epoch 44/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 410ms/step - accuracy: 0.0137 - loss: 3456.9590 - val_accuracy: 0.0023 - val_loss: 4446.4805\n",
      "Epoch 45/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 362ms/step - accuracy: 0.0631 - loss: 3525.5171 - val_accuracy: 0.0038 - val_loss: 4982.2788\n",
      "Epoch 46/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 340ms/step - accuracy: 0.0121 - loss: 3866.5710 - val_accuracy: 0.0046 - val_loss: 4583.3262\n",
      "Epoch 47/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 382ms/step - accuracy: 0.0162 - loss: 4341.2266 - val_accuracy: 0.0077 - val_loss: 6118.8149\n",
      "Epoch 48/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 359ms/step - accuracy: 0.0131 - loss: 4521.1074 - val_accuracy: 0.0292 - val_loss: 5685.2192\n",
      "Epoch 49/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 355ms/step - accuracy: 0.0231 - loss: 5035.7861 - val_accuracy: 0.0131 - val_loss: 6965.1851\n",
      "Epoch 50/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 349ms/step - accuracy: 0.0108 - loss: 4669.8501 - val_accuracy: 0.0462 - val_loss: 5505.3906\n",
      "Epoch 51/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 379ms/step - accuracy: 0.0129 - loss: 5020.4761 - val_accuracy: 0.0077 - val_loss: 8097.8652\n",
      "Epoch 52/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 407ms/step - accuracy: 0.0108 - loss: 5892.3955 - val_accuracy: 0.0223 - val_loss: 7320.4736\n",
      "Epoch 53/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 419ms/step - accuracy: 0.0212 - loss: 6793.8257 - val_accuracy: 0.0038 - val_loss: 8600.6797\n",
      "Epoch 54/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 427ms/step - accuracy: 0.0027 - loss: 6454.4268 - val_accuracy: 0.0462 - val_loss: 7026.5381\n",
      "Epoch 55/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 426ms/step - accuracy: 0.0438 - loss: 5791.9219 - val_accuracy: 0.0077 - val_loss: 8807.7080\n",
      "Epoch 56/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 449ms/step - accuracy: 0.0131 - loss: 6354.5874 - val_accuracy: 0.0462 - val_loss: 8618.1064\n",
      "Epoch 57/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 444ms/step - accuracy: 0.0138 - loss: 6755.1943 - val_accuracy: 0.0769 - val_loss: 9571.1846\n",
      "Epoch 58/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 388ms/step - accuracy: 0.0185 - loss: 7897.5513 - val_accuracy: 0.0038 - val_loss: 11341.8311\n",
      "Epoch 59/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 351ms/step - accuracy: 0.0127 - loss: 9164.4785 - val_accuracy: 0.0077 - val_loss: 11318.6904\n",
      "Epoch 60/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 414ms/step - accuracy: 0.0167 - loss: 8811.0322 - val_accuracy: 0.0062 - val_loss: 11507.0322\n",
      "Epoch 61/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 473ms/step - accuracy: 0.0131 - loss: 9582.9062 - val_accuracy: 0.0769 - val_loss: 12606.6992\n",
      "Epoch 62/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 458ms/step - accuracy: 0.0135 - loss: 10424.4717 - val_accuracy: 0.0038 - val_loss: 13324.2988\n",
      "Epoch 63/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 420ms/step - accuracy: 0.0667 - loss: 9785.5254 - val_accuracy: 0.0077 - val_loss: 13558.4258\n",
      "Epoch 64/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 459ms/step - accuracy: 0.0063 - loss: 10502.2139 - val_accuracy: 0.0138 - val_loss: 12680.9785\n",
      "Epoch 65/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 440ms/step - accuracy: 0.0192 - loss: 11072.2109 - val_accuracy: 0.0069 - val_loss: 13767.9473\n",
      "Epoch 66/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 388ms/step - accuracy: 0.0138 - loss: 10241.9297 - val_accuracy: 0.0000e+00 - val_loss: 14277.4473\n",
      "Epoch 67/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 391ms/step - accuracy: 0.0115 - loss: 12208.4062 - val_accuracy: 0.0000e+00 - val_loss: 16887.9043\n",
      "Epoch 68/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 372ms/step - accuracy: 0.0190 - loss: 13071.9375 - val_accuracy: 7.6923e-04 - val_loss: 18961.4688\n",
      "Epoch 69/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 364ms/step - accuracy: 0.0071 - loss: 14266.3467 - val_accuracy: 0.0769 - val_loss: 17157.0371\n",
      "Epoch 70/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 394ms/step - accuracy: 0.0229 - loss: 12015.5098 - val_accuracy: 0.0054 - val_loss: 16742.0977\n",
      "Epoch 71/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 396ms/step - accuracy: 0.0135 - loss: 11957.1250 - val_accuracy: 0.0069 - val_loss: 14825.4727\n",
      "Epoch 72/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 415ms/step - accuracy: 0.0152 - loss: 13964.5820 - val_accuracy: 0.0100 - val_loss: 20011.8047\n",
      "Epoch 73/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 400ms/step - accuracy: 0.0675 - loss: 15957.6348 - val_accuracy: 0.0131 - val_loss: 15221.6992\n",
      "Epoch 74/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 423ms/step - accuracy: 0.0094 - loss: 14514.5576 - val_accuracy: 0.0046 - val_loss: 20461.7559\n",
      "Epoch 75/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 409ms/step - accuracy: 0.0137 - loss: 17106.5645 - val_accuracy: 0.0769 - val_loss: 18997.2422\n",
      "Epoch 76/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 424ms/step - accuracy: 0.0173 - loss: 14203.7842 - val_accuracy: 0.0100 - val_loss: 20162.7070\n",
      "Epoch 77/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 410ms/step - accuracy: 0.0142 - loss: 16999.3867 - val_accuracy: 0.0146 - val_loss: 25396.8008\n",
      "Epoch 78/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 432ms/step - accuracy: 0.0137 - loss: 21683.0195 - val_accuracy: 0.0000e+00 - val_loss: 26838.9922\n",
      "Epoch 79/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 431ms/step - accuracy: 0.0129 - loss: 18271.6602 - val_accuracy: 0.0038 - val_loss: 23549.1191\n",
      "Epoch 80/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 459ms/step - accuracy: 0.0127 - loss: 18363.5078 - val_accuracy: 7.6923e-04 - val_loss: 26895.1973\n",
      "Epoch 81/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 416ms/step - accuracy: 0.0125 - loss: 19488.7305 - val_accuracy: 0.0038 - val_loss: 23645.8984\n",
      "Epoch 82/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 396ms/step - accuracy: 0.0160 - loss: 19783.8809 - val_accuracy: 0.0054 - val_loss: 25070.3047\n",
      "Epoch 83/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 374ms/step - accuracy: 0.0140 - loss: 21436.9668 - val_accuracy: 0.5392 - val_loss: 29299.4297\n",
      "Epoch 84/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 397ms/step - accuracy: 0.0621 - loss: 22968.6055 - val_accuracy: 0.0131 - val_loss: 27622.1172\n",
      "Epoch 85/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 396ms/step - accuracy: 0.0158 - loss: 23785.5273 - val_accuracy: 0.0131 - val_loss: 30885.4883\n",
      "Epoch 86/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 413ms/step - accuracy: 0.0185 - loss: 27231.5820 - val_accuracy: 0.0000e+00 - val_loss: 28939.4199\n",
      "Epoch 87/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 442ms/step - accuracy: 0.0156 - loss: 25961.7891 - val_accuracy: 0.0769 - val_loss: 36322.1484\n",
      "Epoch 88/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 458ms/step - accuracy: 0.0138 - loss: 30733.7676 - val_accuracy: 0.0031 - val_loss: 38094.4531\n",
      "Epoch 89/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 464ms/step - accuracy: 0.0148 - loss: 28033.6133 - val_accuracy: 0.0000e+00 - val_loss: 36302.5312\n",
      "Epoch 90/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 452ms/step - accuracy: 0.0112 - loss: 28960.4707 - val_accuracy: 0.0000e+00 - val_loss: 36445.9844\n",
      "Epoch 91/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 429ms/step - accuracy: 0.0160 - loss: 30263.1797 - val_accuracy: 0.0138 - val_loss: 38609.3008\n",
      "Epoch 92/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 426ms/step - accuracy: 0.0102 - loss: 30362.7969 - val_accuracy: 0.0100 - val_loss: 36450.6016\n",
      "Epoch 93/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 484ms/step - accuracy: 0.0167 - loss: 31228.0723 - val_accuracy: 0.0108 - val_loss: 39786.0938\n",
      "Epoch 94/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 422ms/step - accuracy: 0.0644 - loss: 32733.3398 - val_accuracy: 0.0038 - val_loss: 39933.4570\n",
      "Epoch 95/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 447ms/step - accuracy: 0.0123 - loss: 32569.8145 - val_accuracy: 0.0215 - val_loss: 45674.1914\n",
      "Epoch 96/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 390ms/step - accuracy: 0.0148 - loss: 35372.7227 - val_accuracy: 0.0000e+00 - val_loss: 41311.3711\n",
      "Epoch 97/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 474ms/step - accuracy: 0.0158 - loss: 36165.1445 - val_accuracy: 0.0100 - val_loss: 51965.6836\n",
      "Epoch 98/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 435ms/step - accuracy: 0.0052 - loss: 40211.1016 - val_accuracy: 0.0000e+00 - val_loss: 37221.6172\n",
      "Epoch 99/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 410ms/step - accuracy: 0.0213 - loss: 36827.4180 - val_accuracy: 0.0062 - val_loss: 59558.0391\n",
      "Epoch 100/100\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 402ms/step - accuracy: 0.0156 - loss: 44474.5938 - val_accuracy: 0.0038 - val_loss: 56095.0391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Dropout, LayerNormalization\n",
    "from keras.layers import MultiHeadAttention, GlobalAveragePooling1D, Add\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Load the datasets\n",
    "data_rom = pd.read_csv(\"csv/data_rom_500.csv\", header=None)  # Romanized text (input)\n",
    "data_kh = pd.read_csv(\"csv/data_kh_500.csv\", header=None)    # Khmer text (target)\n",
    "\n",
    "batch_size = 32  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# Prepare datasets for Roman-to-Khmer\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "for input_text in data_rom[0]:  # Romanized text is now the input\n",
    "    input_text = str(input_text).strip()\n",
    "    input_texts.append(input_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "\n",
    "for target_text in data_kh[0]:  # Khmer text is now the target\n",
    "    target_text = '\\t' + str(target_text).strip() + '\\n'\n",
    "    target_texts.append(target_text)\n",
    "    for char in str(target_text):\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "# Sort characters to ensure consistent token indexing\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "# Create token indices\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# Initialize input and output data for the model\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype=\"float32\")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype=\"float32\")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype=\"float32\")\n",
    "\n",
    "# Populate the data arrays\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # Decoder target data is offset by one timestep\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "\n",
    "# Define Transformer block (Self-Attention and Feed Forward)\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    # Self-Attention Layer\n",
    "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n",
    "    attention = Dropout(dropout)(attention)\n",
    "    attention = LayerNormalization()(attention)\n",
    "    \n",
    "    # Feed Forward Layer\n",
    "    ff = Dense(ff_dim, activation=\"relu\")(attention)\n",
    "    ff = Dropout(dropout)(ff)\n",
    "    ff = Dense(inputs.shape[-1])(ff)\n",
    "    return Add()([inputs, ff])\n",
    "\n",
    "def transformer_decoder(inputs, encoder_output, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    # Self-Attention Layer for Decoder\n",
    "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n",
    "    attention = Dropout(dropout)(attention)\n",
    "    attention = LayerNormalization()(attention)\n",
    "    \n",
    "    # Encoder-Decoder Attention Layer\n",
    "    cross_attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(attention, encoder_output)\n",
    "    cross_attention = Dropout(dropout)(cross_attention)\n",
    "    cross_attention = LayerNormalization()(cross_attention)\n",
    "    \n",
    "    # Feed Forward Layer\n",
    "    ff = Dense(ff_dim, activation=\"relu\")(cross_attention)\n",
    "    ff = Dropout(dropout)(ff)\n",
    "    ff = Dense(inputs.shape[-1])(ff)\n",
    "    return Add()([attention, ff])\n",
    "\n",
    "# Transformer Parameters\n",
    "head_size = 64\n",
    "num_heads = 8\n",
    "ff_dim = 256\n",
    "num_layers = 4\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "x = encoder_inputs\n",
    "for _ in range(num_layers):\n",
    "    x = transformer_encoder(x, head_size, num_heads, ff_dim)\n",
    "encoder_output = x\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "x = decoder_inputs\n",
    "for _ in range(num_layers):\n",
    "    x = transformer_decoder(x, encoder_output, head_size, num_heads, ff_dim)\n",
    "decoder_output = Dense(num_decoder_tokens, activation=\"softmax\")(x)\n",
    "\n",
    "# Define the model that combines encoder and decoder\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"transformer_rom_to_khmer.h5\")\n",
    "\n",
    "# Create inference models\n",
    "encoder_model = Model(encoder_inputs, encoder_output)\n",
    "\n",
    "decoder_state_input = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_output = transformer_decoder(decoder_state_input, encoder_output, head_size, num_heads, ff_dim)\n",
    "decoder_output = Dense(num_decoder_tokens, activation=\"softmax\")(decoder_output)\n",
    "decoder_model = Model([decoder_state_input], decoder_output)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to text\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
